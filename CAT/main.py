import random
from time import sleep

import torch
from sklearn.metrics import matthews_corrcoef, f1_score
from torch import nn
from tqdm import tqdm
import model
import os.path as osp
import os


def start_training(dataset):
    random.shuffle(dataset)
    train_dataset = dataset[:450] # Change based on your data split, if you want to have a validate set, you can also setup a validation set for it.
    test_dataset = dataset[450:] # Change based on your data split, if you want to have a validate set, you can also setup a validation set for it.
    model_ = model.CAT(h_size=128, max_method=5, drop_out_rate=0.5, gcn_layers=3)
    train(epochs=300, trainLoader=train_dataset, testLoader=test_dataset, model=model_, learning_rate=0.0001)


def evaluate_metrics(model, test_loader):
    model.eval()
    with torch.no_grad():
        pred_1, pred_2, pred_3, pred_4, pred_5, pred_6, pred_7 = [], [], [], [], [], [], []
        true_1, true_2, true_3, true_4, true_5, true_6, true_7 = [], [], [], [], [], [], []
        for data in tqdm(test_loader):
            out = model(data)
            for j in range(7):
                exec('pred_{}.append(out[j].argmax(dim=1).tolist())'.format(j+1))
                true_label = data[0].y
                exec('true_{}.append(true_label[j])'.format(j+1))
        print("Confidentiality:")
        print("F-score:", f1_score(true_1, pred_1, average="macro"))
        print("MCC:", matthews_corrcoef(true_1, pred_1))
        print("Integrity:")
        print("F-score:", f1_score(true_2, pred_2, average="macro"))
        print("MCC:", matthews_corrcoef(true_2, pred_2))
        print("Availability:")
        print("F-score:", f1_score(true_3, pred_3, average="macro"))
        print("MCC:", matthews_corrcoef(true_3, pred_3))
        print("Access Complexity:")
        print("F-score:", f1_score(true_4, pred_4, average="macro"))
        print("MCC:", matthews_corrcoef(true_4, pred_4))
        print("Severity:")
        print("F-score:", f1_score(true_5, pred_5, average="macro"))
        print("MCC:", matthews_corrcoef(true_5, pred_5))
        print("Access Vector:")
        print("F-score:", f1_score(true_6, pred_6, average="macro"))
        print("MCC:", matthews_corrcoef(true_6, pred_6))
        print("Authentication:")
        print("F-score:", f1_score(true_7, pred_7, average="macro"))
        print("MCC:", matthews_corrcoef(true_7, pred_7))


def train(epochs, trainLoader, testLoader, model, learning_rate):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    model.train()
    try:
        for e in range(epochs):
            for index, _data in enumerate(tqdm(trainLoader, leave=False)):
                model.train()
                out = model(_data)
                y_1 = torch.reshape(_data[0].y[0].clone().detach(), (-1,))
                y_2 = torch.reshape(_data[0].y[1].clone().detach(), (-1,))
                y_3 = torch.reshape(_data[0].y[2].clone().detach(), (-1,))
                y_4 = torch.reshape(_data[0].y[3].clone().detach(), (-1,))
                y_5 = torch.reshape(_data[0].y[4].clone().detach(), (-1,))
                y_6 = torch.reshape(_data[0].y[5].clone().detach(), (-1,))
                y_7 = torch.reshape(_data[0].y[6].clone().detach(), (-1,))
                out_1 = out[0].clone().detach().requires_grad_(True)
                out_2 = out[1].clone().detach().requires_grad_(True)
                out_3 = out[2].clone().detach().requires_grad_(True)
                out_4 = out[3].clone().detach().requires_grad_(True)
                out_5 = out[4].clone().detach().requires_grad_(True)
                out_6 = out[5].clone().detach().requires_grad_(True)
                out_7 = out[6].clone().detach().requires_grad_(True)
                loss = torch.autograd.Variable((criterion(out_1, y_1) + criterion(out_2, y_2) + criterion(out_3, y_3) + criterion(out_4, y_4) + criterion(out_5, y_5) + criterion(out_6, y_6) + criterion(out_7, y_7))/7, requires_grad = True)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                sleep(0.05)
                if index % 20 == 0:
                    print('epoch: {}, batch: {}, loss: {}'.format(e + 1, index + 1, loss.data))
            exec('torch.save(model, os.getcwd() + "//model//" + "model_{}.pt")'.format(e + 1))
            sleep(0.1)
        evaluate_metrics(model=model, test_loader=testLoader)
    except KeyboardInterrupt:
        evaluate_metrics(model=model, test_loader=testLoader)


def demo_work(dataset):
    model_ = torch.load("model.pt")
    test_dataset = dataset
    print("On the demo test dataset: ")
    evaluate_metrics(model=model_, test_loader=test_dataset)


if __name__ == '__main__':
    dataset = []
    for i in range(500): # Change based on your dataset size.
        data = torch.load(osp.join(os.getcwd() + "\\data\\", 'data_{}.pt'.format(i)))
        dataset.append(data)
    start_training(dataset)
